---
title: "ProblemSet2"
author: "Aaron Graybill"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    toc: true
  html_document: 
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(here)
library(dplyr)
library(tidyr)
library(ggplot2)

source(here("R-scripts","ggplotTheme copy.R"))
```

## Problem 2.1

```{r 2.1 data pull}
df <- 
  read.csv(here('Data',"playbill.csv"))
movie_lm <- 
  lm(CurrentWeek~LastWeek,data=df)
summary(movie_lm)
```

### 2.1.a
We can find the 95% confidence interval on the $\hat{\beta}_1$ term with the following code:
```{r 2.1.a}
confint_2.1 <- 
  confint(movie_lm)
```

The confidence interval around $\hat{\beta}_1$ is $(`r confint_2.1[2,]`)$ which actually contains $1$, so we do not have significant evidence that the true $\beta_1\neq1$. We can compute this test a different way. We can run a hypothesis test where $H_0\colon \beta_1=1$ and $H_a\colon \beta_1\neq1$. We showed in class that:
$$
\frac{\hat{\beta}_1-1}{se(\hat{\beta}_1)}\sim t_{n-2}
$$
We have all of these values from our regression output, so we can compute the $p$-value that the null hypothesis is true in the following way:

```{r 2.1.b 1}
test_stat=(movie_lm[["coefficients"]][["LastWeek"]]-1)/coef(summary(movie_lm))[, "Std. Error"]["LastWeek"]

critical_region <- 
  qt(c(.025,.975),movie_lm$df.residual)

p_val <- (pt(test_stat,movie_lm$df.residual)*2)
  
```

The 95% critical region, as computed is $(-\infty,-2.12)\cup(2.12,\infty)$, and our test statistic is equal to $`r test_stat`$ is not in that region, so we have insufficient evidence to reject the null hypothesis that $\beta_1=1$ at the 95% level. In fact, the $p$-value from this test is $`r p_val`$ which is well above the required $.05$ at the 95% level.

### 2.1.b

As proven in class:
$$
\frac{\hat{\beta}_0-\beta_0^0}{se(\hat{\beta}_0)}\sim t_{n-2}
$$
In this case our $H_0$ is $\beta_0=1000$ and $H_a$ is $\beta_0\neq1000$. Implementing similar code to above gives:

```{r 2.1.b 2}
test_stat=(movie_lm[["coefficients"]][["(Intercept)"]]-1000)/coef(summary(movie_lm))[, "Std. Error"]["(Intercept)"]

critical_region <- 
  qt(c(.025,.975),movie_lm$df.residual)

p_val <- (pt(-abs(test_stat),movie_lm$df.residual)*2)
```

I have to do some business with `-abs(test_stat)` to ensure that when I compute the cumulative density it's in the left tail so the $p$-value is just two times the computed density. Anyway, computing the $p$-value gives: `r p_val` which is quite high and provides very little evidence that the true $\beta_0\neq1000$. In fact since $p>.5$, there is more evidence that $H_0$ is true than the alternative.

### 2.1.c
We showed in class that the prediction interval is given by:
$$
\hat{\beta}_0+\hat{\beta}_1x^*\pm t_{\frac{\alpha}{2},n-2}S\sqrt{\frac{1}{n}+\frac{x^*-\overline{x}}{SXX}}
$$
But thankfully `R`will take care of that for us with the following code:
```{r 2.1.c}
prediction_points <- 
  data.frame(LastWeek=400000)
prediction <- 
  predict(movie_lm,prediction_points,interval = 'prediction',level=.95,se.fit=T)
prediction
```
Summarizing those results, the point estimate for `CurrentWeek` is $\$`r prediction[['fit']][1] %>% format(scientific=F)`$ with a 95% prediction interval of: $(`r prediction[['fit']][2] %>% format(scientific=F)`,`r prediction[['fit']][3] %>% format(scientific=F)`)$. So we are 95% percent certain the true value of $y^*$ would lie in the aforementioned range.


### 2.1.d
This heuristic is more or less appropriate. The regression coefficient is $\hat{\beta}_1=`r movie_lm$coefficients['LastWeek']`$ which says that sales next week will be approximately 98% what they were last week which is quite close to exactly what they were last week. That 2% difference might not be a problem for some, but the true estimate is not exactly one. In fact, we could not conclusively show that the $\beta_1$ was different from 1, the $p$-value on that test was $.23$, not significant evidence to the contrary.


## Problem 2.2
```{r 2.2 read data}
df <- 
  read.delim(here('Data',"indicators.txt"),sep = '\t')
```

### 2.2.a
I create the linear model and the confidence in the following way:
```{r 2.2 create model}
econ_lm <- 
  lm(PriceChange~LoanPaymentsOverdue,data=df)
summary(econ_lm)
confint(econ_lm)
```
The 95% confidence interval on the $\hat{\beta}_1$ does not contain any positive values so we can be confident that the true slope, $\beta_1$, also is not positive. We could also run a hypothesis test to the same effect. Interpreting this, we can be reasonably certain that an increase in overdue loan payments is associated with a decrease in prices.

### 2.2.b
Here we are not doing a prediction interval, we are doing a confidence interval on the expected value of $Y$ given $x=4$. We implement that in the following way:
```{r 2.2.b}
data <- data.frame(LoanPaymentsOverdue=4)
predict(econ_lm,data,interval = 'confidence',se.fit=T,level = .95)
```
The expected value of change in prices is $-4.48\%$ when $x=4$ which is the percentage of loans overdue. The confidence interval does not include zero, so we can be reasonably sure that the expected value of price change is not zero.

## Problem 2.4

### 2.4.a
We wish to minimize the square residuals and solve for the $\hat{\beta}$ that does so.

The sum of the square residuals are $\sum_i^n\left(y_i-\hat{y}_i\right)^2$. And our model is of the form: $\hat{y}_i=\hat{\beta}x_i$ so we have:

$$
\arg\min_{\hat{\beta}}\left\{\sum_i^n\left(y_i-\hat{\beta}x_i\right)^2 \right\}
$$

The first order condition would then be:

$$
\sum_i^n-2\left(y_i-\hat{\beta}x_i\right)x_i=0
$$
Before solving for $\hat{\beta}$, let's take another derivative, giving $\sum x_i^2$ which is greater than or equal to zero, so when we solve for $0$ in the first order condition, we can be sure we are finding a minimum
Doing some algebraic manipulations gives:

\begin{align*}
\sum_i^n-2\left(y_i-\hat{\beta}x_i\right)x_i&=0\\
-2\sum_i^n x_iy_i-\hat{\beta}x_i^2&=0\\
\sum_i^n x_iy_i-\hat{\beta}x_i^2&=0 \\
\sum_i^n x_iy_i-\sum_i^n\hat{\beta}x_i^2&=0\\
\sum_i^n x_iy_i-\hat{\beta}\sum_i^nx_i^2&=0\\
\sum_i^n x_iy_i&=\hat{\beta}\sum_i^nx_i^2\\
\frac{\sum_i^n x_iy_i}{\sum_i^nx_i^2}&=\hat{\beta}\\
\end{align*}

We know that the function has attained a minimum and not a maximum because the function is convex as it is only the sum of squares.
### 2.4.b

#### i.

\begin{align*}
E[\hat{\beta}|X=x_i]&=\\
&=E\left[\frac{\sum_i^n x_iy_i}{\sum_i^nx_i^2}|X=x_i\right]\\
&=\frac{E\left[\sum_i^n x_iy_i|X=x_i\right]}{E\left[\sum_i^nx_i^2|X=x_i\right]}\\
&=\frac{\sum_i^n x_iE\left[y_i|X=x_i\right]}{\sum_i^nx_i^2} \text{(conditioning on}\ x)\\
&=\frac{\sum_i^n x_i\beta x_i}{\sum_i^nx_i^2} \text{(by assumption)}\\
&=\beta\frac{\sum_i^n x_i^2}{\sum_i^nx_i^2}\\
&=\beta
\end{align*}


#### ii.

\begin{align*}
Var[\hat{\beta}|X=x_i]&=\\
&=Var\left[\frac{\sum_i^n x_iy_i}{\sum_i^nx_i^2}|X=x_i\right]\\
&=\frac{1}{\left(\sum_i^nx_i^2\right)^2}Var(\sum_i^n x_iy_i|X=x_i)\quad \text{(conditioning on} \ x)\\
&=\frac{1}{\left(\sum_i^nx_i^2\right)^2}\sum_i^n x_i^2 Var(y_i|X=x_i) \quad \mathbf{1.}\\
&=\frac{1}{\left(\sum_i^nx_i^2\right)^2}\sum_i^n x_i^2 Var(\beta x_i+e_i|X=x_i)\quad  \text{(modelling assumption)}\\
&=\frac{1}{\left(\sum_i^nx_i^2\right)^2}\sum_i^n x_i^2 \sigma^2 \quad \mathbf{2.}\\
&=\frac{\sigma^2\sum_i^nx_i^2}{\left(\sum_i^nx_i^2\right)^2}\\
&=\frac{\sigma^2}{\sum_i^nx_i^2}
\end{align*}

The $\mathbf{1.}$ step uses the conditioning on $X=x_i$ and the fact that the $Y_i$s are independent. Step $\mathbf{2.}$ uses the conditioning on $x$ coupled with the fact that we then only have a location shift so the variance is unchanged and finally the modeling assumption that $Var(e_i|X=x_i)=\sigma^2$.

#### iii.
We have already shown that the mean and variance of $\hat{\beta}$  are as desired, now just to prove normality. As shown $\hat{\beta}=\frac{\sum x_iy_i}{\sum x_i^2}$. Conditioning on $X$ means the denominator is a constant. Furthermore, Conditioning on $X$ means that the numerator is the weighted sum of a series of normal distribution (because $y|X\sim N$). Since all of the $y_i$s are uncorrelated (and independent) by assumption, this weighted sum of normals must remain a normal distribution. Therefore, we have a normal distribution divided by a constant which itself must be a normal. Therefore, we have proven that $\hat{\beta}$ is distributed normally and since we already know its two parameters, we can fully characterize the distribution of $\hat{\beta}$ as $\hat{\beta}\sim N\left(\beta,\frac{\sigma^2}{\sum_i^nx_i^2}\right)$. $\blacksquare$





